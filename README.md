# 1 Introduction

This submission investigates:

1. Data Pre-processing Techniques: Analysis of different methods to clean, transform, and prepare data for modeling.
2. Different Models/Algorithms: Exploration of various machine learning models and algorithms to address the problem.
3. Hyperparameter Optimization: Techniques for optimizing model hyperparameters for better performance.
4. Fine-tuning vs Full Training: Comparison between fine-tuning pre-trained models and training models from scratch.

# Requirements
To run the experiments and analysis, you need to install the following dependencies:

- datasets: A library for loading and working with datasets in various formats.
- transformers: A library for state-of-the-art natural language processing models, transformers, and tokenizers.
- spacy: A library for advanced natural language processing tasks.
- torch: A deep learning library used for building and training neural networks.
- spacy-transformers: A library that allows spaCy to work with transformer models.
- seqeval: A library for evaluating sequence tagging models.
- scikit-learn: The scikit-learn library, used for machine learning and data analysis.
- numpy: A numerical computing library for working with arrays and mathematical operations.

You can install these dependencies using the following command:
*pip install -r requirements.txt*


# Functions
The functions are contained in utilities.py and cover a range of tasks such as data processing and evaluation.

# Running the experiments/analysis
1. Data Preparation: Run Data Preparation.ipynb to format the dataset and integrate it into your Google Drive.
2. Running Experiments: Once data preparation is complete, you can run the experiments. Refer to individual experiment notebooks for instructions.